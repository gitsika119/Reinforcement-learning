{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUeU7oawKmgH/oKaUvcBmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DS-Aditya-928/CartPoleProject4/blob/main/RL_Project4_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YBUjQ96xxib"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Imports! The first 3 are for our cartpole simulation, numpy is for our bot, tqdm is a super easy way to\n",
        "draw progress bars, and the last one is used to play the video of the simulation.\n",
        "\n",
        "'''\n",
        "import gymnasium as gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import moviepy.editor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ“Œ Before we go any further:\n",
        "\n",
        "Let's have a look over what we're trying to accomplish.\n",
        "\n",
        "<br>\n",
        "\n",
        "Reinforcement learning is a machine learning algorithm where we let the algorithm make its own descisions in an attempt to maximize a \"reward\". This reward can be a high score in a game, or more relevant to our project here, the time our algorithm is able to balance a pole mounted on a cart for.\n",
        "\n",
        "\n",
        "To train our algorithm, we're going to be using something called Q learning.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#ðŸ“Œ What is Q-learning?\n",
        "\n",
        "This bit is pretty complex, and this video explains Q learning better than I ever could :p\n",
        "\n",
        "https://www.youtube.com/watch?v=TiAXhVAZQl8  \n",
        "  \n",
        "<br>\n",
        "\n",
        "The basic idea however, is that we are going to maintain a table (appropriately called a Q table) that holds expected changes in score for all of our possible actions at a give state.\n",
        "\n",
        "<br>\n",
        "\n",
        "I.E, for the cart pole example, let's say the pole is 5 degrees off centre, and has a velocity of 1 m/s. Our algorithm is going to go to the corresponding cell in our table, and see which one of the possible actions (in the cartpole example, those are moving left or right) will result in the greatest increase in score. So, the cell holds an array with expected score changes for each action.\n",
        "\n",
        "Let's say the cell for the case described above has this array:  [-1.0, +2.5], with the first value corresponding to the expected change in score if we move to the left, and the other if we move to the right. Moving to the right gives us a score change of +2.5, so we're going to move to the right.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "The training bit here involves having the model change these values in the q table so that it makes better and better decisions over time. We'll delve more into the specifics of how to accomplish this later.\n",
        "\n",
        "<br>\n",
        "Got all that? It's ok if you didn't. I'm still wrapping my head around it myself. Feel free to ask us questions during office hours and USE THE INTERNET. It can teach you anything if you use it right.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qz_KfKuDZdPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This here is the CartPoleBot class; all the functions you'll need to implement are in here.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTVEvKm8U9-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "class CartPoleBot:\n",
        "\n",
        "  env:gym.Env\n",
        "  learningRate:float\n",
        "  discountFactor:float\n",
        "\n",
        "  def __init__(self, env: gym.Env, learningRate: float,\n",
        "               initalEpsilon: float, epsilonDecay: float, finalEpsilon: float,\n",
        "               discountFactor: float):\n",
        "    '''\n",
        "    Constructor. Don't change anything here. READ ALL THE COMMENTS THOUGH, they're hella useful.\n",
        "    '''\n",
        "    self.env = env #Our cartpole environment.\n",
        "\n",
        "    self.learningRate = learningRate #The rate at which we update values in our Q-table.\n",
        "\n",
        "    self.epsilon = initalEpsilon\n",
        "    self.epsilonDecay = epsilonDecay\n",
        "    self.finalEpsilon = finalEpsilon\n",
        "\n",
        "    '''\n",
        "    Does the word epsilon scare you? Fortunately, it's a fairly simple concept.\n",
        "\n",
        "    At first, the values in the Q-table are going to be very wrong, so it makes sense to\n",
        "    have the algorithm make random decisions instead, then take note of what happens and use that\n",
        "    information to update our Q-table. We'll get into how we update the Q-table later.\n",
        "\n",
        "    Whether or not our algorithm makes a random decision or looks up the Q-table value depends on epsilon.\n",
        "    Specifically, we are going to generate a random number between 0 and 1 and check if epsilon is higher than\n",
        "    it. If it is, we pick a random action. If epsilon is lower than the random value, we look up the Q-table value.\n",
        "\n",
        "    So, our initial epsilon is going to be a fairly high value (epsilon is between 0 and 1)\n",
        "    which means that we're going to make more random decisions at the start, but we're going to lower it over time\n",
        "    as our algorithm learns, preferring the values from the (hopefully now correct) Q-table.\n",
        "\n",
        "    Initial epsilon and final epsilon are both self explanatory, and epsilonDecay is the rate at which\n",
        "    epsilon decreases.\n",
        "    '''\n",
        "\n",
        "    self.qTable = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
        "\n",
        "    '''\n",
        "    the line above creates our Q table!\n",
        "    #The idea is this: when we look up the specific state we are at in the dictionary\n",
        "    #above, we'll have an array that has expected score changes for each action. (That's why each array\n",
        "    # is env.action_space.n long. the action_space holds all possible actions, and .n returns its length.)\n",
        "    #So if going left is action 0, then the value at index 0 in the aforementioned array is\n",
        "    #the expected score change from going left.\n",
        "    '''\n",
        "\n",
        "    self.discountFactor = discountFactor#we'll get into what this does later.\n",
        "\n",
        "  def discConv(self, obs):\n",
        "    '''\n",
        "    When we look up values in the Q-Table (represented as a dictionary), the key\n",
        "    will be the state of the simulation, which is a numpy array.\n",
        "\n",
        "    This sounds straightforward enough, but there are a few challenges:\n",
        "\n",
        "    1.) The simulation values take the form of a numpy array, which is non-hashable and thus\n",
        "    can't be used as a key for our dictionary.\n",
        "\n",
        "    2.) Because there are infinite number of numbers, it's highly unlikely that we will\n",
        "    run into the exact same values more than once (if at all). So, we shall generalise a bit\n",
        "    and break down each range of sim values into discrete chunks. I've chosen 10 chunks for\n",
        "    each here, but feel free to experiment.\n",
        "\n",
        "    This function takes a numpy array representing the simulation state and returns a\n",
        "    hashable tuple, with the values \"rounded\" to the closest chunk, keeping us from having multiple\n",
        "    Q-Table entries for values that are very close but not exactly the same.\n",
        "    '''\n",
        "    #DO NOT CHANGE.\n",
        "    posSpace = np.linspace(-2.4, 2.4, 10)\n",
        "    velSpace = np.linspace(-4, 4, 10)\n",
        "    angSpace = np.linspace(-.2095, .2095, 10)\n",
        "    angVSpace = np.linspace(-4, 4, 10)\n",
        "    lTodArray = [posSpace, velSpace, angSpace, angVSpace]\n",
        "    tR = []\n",
        "    for i in range(len(obs)):\n",
        "      tR += [np.digitize(obs[i], lTodArray[i])]\n",
        "\n",
        "    return(tuple(tR))\n",
        "\n",
        "  def getAction(self, observation):\n",
        "    #TO DO\n",
        "    '''\n",
        "    Your job here is to generate a random number, check if it's higher than epsilon and\n",
        "    then, based on that, choose a random action or look up the Q-table's reccomended action.\n",
        "\n",
        "    Use numpys random function to generate a random number.\n",
        "    Here's the gymnasium library documentation: https://gymnasium.farama.org/. It should tell you how to\n",
        "    get a random action.\n",
        "    '''\n",
        "\n",
        "    #Your code here.\n",
        "    pass\n",
        "\n",
        "  def update(self, pastObv, action, reward, terminated, currObv):\n",
        "    '''\n",
        "    This is where we put everything we learned about Q-learning into practice.\n",
        "\n",
        "    Just so we're clear, when this function is called, we've already taken an action based on what getAction\n",
        "    returned, and we're now adjusting our q values based on how good/bad said action was.\n",
        "\n",
        "    First, let's go over inputs:\n",
        "    pastObv: State of the simulation(i.e angle of the pole, velocity etc.) BEFORE we took an action.\n",
        "    action: The action we TOOK (generated by getAction).\n",
        "    reward: the reward given to us by the environment.\n",
        "    terminated: Whether the simulation ended or not because we failed (truncated is when the simulation ends\n",
        "                because we balanced the pole for long enough).\n",
        "    currObv: The state of the simulation AFTER the action from getAction was taken.\n",
        "\n",
        "    In a broad sense, what we are trying to do here is calculate new values for a given\n",
        "    cell in the Q-Table, and then make a small adjustment to the existing values accordingly.\n",
        "\n",
        "    To do this, let's first calculate the temporal difference.\n",
        "\n",
        "    temporalDiff = (reward for the current state) + (max(qTable[currObv])) - qTable[pastObv][action]\n",
        "\n",
        "    The temporal difference is the difference between the q table value of the old position (before action),\n",
        "    and the value based on the reward and the maximum increase in score that we can obtain by making a move from\n",
        "    our new position (after the action) (NOTE: This value is 0 if the simulation ended!). Note that the\n",
        "    maximum increase from this point isn't given equal weightage. Instead, we shall multiply it\n",
        "    by the discontFactor before using it to calculate the temporalDiff. This is because we want our model to\n",
        "    make decisions that are better in the short term as opposed to a possible reward gain another move later.\n",
        "\n",
        "    The sum of the reward and max increase is what the q value should actually be,\n",
        "    so we adjust qTable[pastObv][action] by the temporal diff * learning rate.\n",
        "\n",
        "    Again, the video linked at the beginning is a must watch IMO, it'll make everything way easier to visualize.\n",
        "    '''\n",
        "    pastObv = self.discConv(pastObv)\n",
        "    currObv = self.discConv(currObv)\n",
        "\n",
        "    #Your code here:\n",
        "    pass\n",
        "\n",
        "  def decayEpsilon(self):\n",
        "    #TO DO\n",
        "    '''\n",
        "    This is pretty easy; when this function is called, you're going to decrease\n",
        "    epsilon by epsilonDecay. But remember, there is a minimum value that epsilon cannot drop below.\n",
        "    '''\n",
        "\n",
        "    #Your code here:\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "E6Cd3AeUerbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode = \"rgb_array\"), \"/content\", episode_trigger= lambda x: (x%5000 == 0), new_step_api= True)#Generate a .mp4 video of our simulation every 5000 episodes.\n",
        "\n",
        "#Hyperparameters\n",
        "#these values aren't.... great, but they do get the job done eventually. I definitely recommend fiddling with these.\n",
        "learningRate = 0.05\n",
        "nEps = 60_000\n",
        "startEpsilon = 1.0\n",
        "epsilonDecay = (1.0/30_000.0)\n",
        "finalEpsilon = 0.1\n",
        "discountFactor = 0.95\n",
        "\n",
        "#Initialize the bot.\n",
        "balanceAgent = CartPoleBot(env, learningRate, startEpsilon, epsilonDecay, finalEpsilon, discountFactor)\n",
        "\n",
        "for i in tqdm(range(nEps)):\n",
        "  observation, info = env.reset()#reset the environment at the start of every episode\n",
        "\n",
        "  done = False\n",
        "  while not done:#We're done once we either fail (terminated) or pass (truncated)\n",
        "    action = balanceAgent.getAction(observation)\n",
        "    newObv, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    balanceAgent.update(observation, action, reward, terminated, newObv)\n",
        "\n",
        "    done = terminated or truncated\n",
        "    observation = newObv\n",
        "\n",
        "  balanceAgent.decayEpsilon()#Epsilon is reduced every episode.\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "d-okFDFJyJ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#call this to play one of the generated mp4s. Replace N with the episode count. Or just download it idk im not ur dad\n",
        "moviepy.editor.ipython_display(\"/content/rl-video-episode-N.mp4\")"
      ],
      "metadata": {
        "id": "daKctNGlvixI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}