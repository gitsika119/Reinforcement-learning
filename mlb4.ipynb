{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzhH5OVr0p25",
        "outputId": "2184b886-03fd-4b04-c02b-2d9e497df789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: DeprecationWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "100%|██████████| 60000/60000 [57:31<00:00, 17.38it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "MoviePy error: the file /content/videos/rl-video-episode-1.mp4 could not be found!\nPlease check that you entered the correct path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b08d3ab29ae9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;31m#call this to play one of the generated mp4s. Replace N with the episode count. Or just download it idk im not ur dad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/videos/rl-video-episode-1.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/moviepy/video/io/html_tools.py\u001b[0m in \u001b[0;36mipython_display\u001b[0;34m(clip, filetype, maxduration, t, fps, rd_kwargs, center, **html_kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ImageClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     return HTML2(html_embed(clip, filetype=filetype, maxduration=maxduration,\n\u001b[0m\u001b[1;32m    221\u001b[0m                 center=center, rd_kwargs=rd_kwargs, **html_kwargs))\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/moviepy/video/io/html_tools.py\u001b[0m in \u001b[0;36mhtml_embed\u001b[0;34m(clip, filetype, maxduration, rd_kwargs, center, **html_kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'audio'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_parse_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxduration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             raise ValueError(\"The duration of video %s (%.1f) exceeds the 'maxduration' \"%(filename, duration)+\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file /content/videos/rl-video-episode-1.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "Imports! The first 3 are for our cartpole simulation, numpy is for our bot, tqdm is a super easy way to\n",
        "draw progress bars, and the last one is used to play the video of the simulation.\n",
        "\n",
        "'''\n",
        "import gymnasium as gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import moviepy.editor\n",
        "from collections import defaultdict\n",
        "class CartPoleBot:\n",
        "\n",
        "  env:gym.Env\n",
        "  learningRate:float\n",
        "  discountFactor:float\n",
        "\n",
        "  def __init__(self, env: gym.Env, learningRate: float,\n",
        "               initalEpsilon: float, epsilonDecay: float, finalEpsilon: float,\n",
        "               discountFactor: float):\n",
        "    '''\n",
        "    Constructor. Don't change anything here. READ ALL THE COMMENTS THOUGH, they're hella useful.\n",
        "    '''\n",
        "    self.env = env #Our cartpole environment.\n",
        "\n",
        "    self.learningRate = learningRate #The rate at which we update values in our Q-table.\n",
        "\n",
        "    self.epsilon = initalEpsilon\n",
        "    self.epsilonDecay = epsilonDecay\n",
        "    self.finalEpsilon = finalEpsilon\n",
        "\n",
        "    '''\n",
        "    Does the word epsilon scare you? Fortunately, it's a fairly simple concept.\n",
        "\n",
        "    At first, the values in the Q-table are going to be very wrong, so it makes sense to\n",
        "    have the algorithm make random decisions instead, then take note of what happens and use that\n",
        "    information to update our Q-table. We'll get into how we update the Q-table later.\n",
        "\n",
        "    Whether or not our algorithm makes a random decision or looks up the Q-table value depends on epsilon.\n",
        "    Specifically, we are going to generate a random number between 0 and 1 and check if epsilon is higher than\n",
        "    it. If it is, we pick a random action. If epsilon is lower than the random value, we look up the Q-table value.\n",
        "\n",
        "    So, our initial epsilon is going to be a fairly high value (epsilon is between 0 and 1)\n",
        "    which means that we're going to make more random decisions at the start, but we're going to lower it over time\n",
        "    as our algorithm learns, preferring the values from the (hopefully now correct) Q-table.\n",
        "\n",
        "    Initial epsilon and final epsilon are both self explanatory, and epsilonDecay is the rate at which\n",
        "    epsilon decreases.\n",
        "    '''\n",
        "\n",
        "    self.qTable = defaultdict(lambda: np.zeros(self.env.action_space.n))\n",
        "\n",
        "    '''\n",
        "    the line above creates our Q table!\n",
        "    #The idea is this: when we look up the specific state we are at in the dictionary\n",
        "    #above, we'll have an array that has expected score changes for each action. (That's why each array\n",
        "    # is env.action_space.n long. the action_space holds all possible actions, and .n returns its length.)\n",
        "    #So if going left is action 0, then the value at index 0 in the aforementioned array is\n",
        "    #the expected score change from going left.\n",
        "    '''\n",
        "\n",
        "    self.discountFactor = discountFactor#we'll get into what this does later.\n",
        "\n",
        "  def discConv(self, obs):\n",
        "    '''\n",
        "    When we look up values in the Q-Table (represented as a dictionary), the key\n",
        "    will be the state of the simulation, which is a numpy array.\n",
        "\n",
        "    This sounds straightforward enough, but there are a few challenges:\n",
        "\n",
        "    1.) The simulation values take the form of a numpy array, which is non-hashable and thus\n",
        "    can't be used as a key for our dictionary.\n",
        "\n",
        "    2.) Because there are infinite number of numbers, it's highly unlikely that we will\n",
        "    run into the exact same values more than once (if at all). So, we shall generalise a bit\n",
        "    and break down each range of sim values into discrete chunks. I've chosen 10 chunks for\n",
        "    each here, but feel free to experiment.\n",
        "\n",
        "    This function takes a numpy array representing the simulation state and returns a\n",
        "    hashable tuple, with the values \"rounded\" to the closest chunk, keeping us from having multiple\n",
        "    Q-Table entries for values that are very close but not exactly the same.\n",
        "    '''\n",
        "    #DO NOT CHANGE.\n",
        "    posSpace = np.linspace(-2.4, 2.4, 10)\n",
        "    velSpace = np.linspace(-4, 4, 10)\n",
        "    angSpace = np.linspace(-.2095, .2095, 10)\n",
        "    angVSpace = np.linspace(-4, 4, 10)\n",
        "    lTodArray = [posSpace, velSpace, angSpace, angVSpace]\n",
        "    tR = []\n",
        "    for i in range(len(obs)):\n",
        "      tR += [np.digitize(obs[i], lTodArray[i])]\n",
        "\n",
        "    return(tuple(tR))\n",
        "\n",
        "  def getAction(self, observation):\n",
        "    #TO DO\n",
        "    '''\n",
        "    Your job here is to generate a random number, check if it's higher than epsilon and\n",
        "    then, based on that, choose a random action or look up the Q-table's reccomended action.\n",
        "\n",
        "    Use numpys random function to generate a random number.\n",
        "    Here's the gymnasium library documentation: https://gymnasium.farama.org/. It should tell you how to\n",
        "    get a random action.\n",
        "    '''\n",
        "    if np.random.rand() < self.epsilon:  # Exploration: choose a random action\n",
        "            return self.env.action_space.sample()\n",
        "    else:  # Exploitation: choose the action with the highest Q-value\n",
        "            state = self.discConv(observation)\n",
        "    return np.argmax(self.qTable[state])\n",
        "    #Your code here.\n",
        "    pass\n",
        "\n",
        "  def update(self, pastObv, action, reward, terminated, currObv):\n",
        "    '''\n",
        "    This is where we put everything we learned about Q-learning into practice.\n",
        "\n",
        "    Just so we're clear, when this function is called, we've already taken an action based on what getAction\n",
        "    returned, and we're now adjusting our q values based on how good/bad said action was.\n",
        "\n",
        "    First, let's go over inputs:\n",
        "    pastObv: State of the simulation(i.e angle of the pole, velocity etc.) BEFORE we took an action.\n",
        "    action: The action we TOOK (generated by getAction).\n",
        "    reward: the reward given to us by the environment.\n",
        "    terminated: Whether the simulation ended or not because we failed (truncated is when the simulation ends\n",
        "                because we balanced the pole for long enough).\n",
        "    currObv: The state of the simulation AFTER the action from getAction was taken.\n",
        "\n",
        "    In a broad sense, what we are trying to do here is calculate new values for a given\n",
        "    cell in the Q-Table, and then make a small adjustment to the existing values accordingly.\n",
        "\n",
        "    To do this, let's first calculate the temporal difference.\n",
        "\n",
        "    temporalDiff = (reward for the current state) + (max(qTable[currObv])) - qTable[pastObv][action]\n",
        "\n",
        "    The temporal difference is the difference between the q table value of the old position (before action),\n",
        "    and the value based on the reward and the maximum increase in score that we can obtain by making a move from\n",
        "    our new position (after the action) (NOTE: This value is 0 if the simulation ended!). Note that the\n",
        "    maximum increase from this point isn't given equal weightage. Instead, we shall multiply it\n",
        "    by the discontFactor before using it to calculate the temporalDiff. This is because we want our model to\n",
        "    make decisions that are better in the short term as opposed to a possible reward gain another move later.\n",
        "\n",
        "    The sum of the reward and max increase is what the q value should actually be,\n",
        "    so we adjust qTable[pastObv][action] by the temporal diff * learning rate.\n",
        "\n",
        "    Again, the video linked at the beginning is a must watch IMO, it'll make everything way easier to visualize.\n",
        "    '''\n",
        "    pastObv = self.discConv(pastObv)\n",
        "    currObv = self.discConv(currObv)\n",
        "\n",
        "    #Your code here:\n",
        "    # Temporal Difference (TD) error\n",
        "    maxFutureQ = np.max(self.qTable[currObv])  # The max Q-value for the next state\n",
        "    temporalDiff = reward + self.discountFactor * maxFutureQ - self.qTable[pastObv][action]\n",
        "    # Update Q-value for the past state-action pair\n",
        "    self.qTable[pastObv][action] += self.learningRate * temporalDiff\n",
        "    pass\n",
        "\n",
        "  def decayEpsilon(self):\n",
        "    #TO DO\n",
        "    '''\n",
        "    This is pretty easy; when this function is called, you're going to decrease\n",
        "    epsilon by epsilonDecay. But remember, there is a minimum value that epsilon cannot drop below.\n",
        "    '''\n",
        "\n",
        "    #Your code here:\n",
        "    self.epsilon = max(self.finalEpsilon, self.epsilon - self.epsilonDecay)\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "env = RecordVideo(gym.make(\"CartPole-v1\", render_mode = \"rgb_array\"), \"/content/videos\", episode_trigger= lambda x: (x%5000 == 0), new_step_api= True)#Generate a .mp4 video of our simulation every 5000 episodes.\n",
        "\n",
        "#Hyperparameters\n",
        "#these values aren't.... great, but they do get the job done eventually. I definitely recommend fiddling with these.\n",
        "learningRate = 0.05\n",
        "nEps = 60_000\n",
        "startEpsilon = 1.0\n",
        "epsilonDecay = (1.0/30_000.0)\n",
        "finalEpsilon = 0.1\n",
        "discountFactor = 0.95\n",
        "\n",
        "#Initialize the bot.\n",
        "balanceAgent = CartPoleBot(env, learningRate, startEpsilon, epsilonDecay, finalEpsilon, discountFactor)\n",
        "\n",
        "for i in tqdm(range(nEps)):\n",
        "  observation, info = env.reset()#reset the environment at the start of every episode\n",
        "\n",
        "  done = False\n",
        "  while not done:#We're done once we either fail (terminated) or pass (truncated)\n",
        "    action = balanceAgent.getAction(observation)\n",
        "    newObv, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    balanceAgent.update(observation, action, reward, terminated, newObv)\n",
        "\n",
        "    done = terminated or truncated\n",
        "    observation = newObv\n",
        "\n",
        "  balanceAgent.decayEpsilon()#Epsilon is reduced every episode.\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "#call this to play one of the generated mp4s. Replace N with the episode count. Or just download it idk im not ur dad\n",
        "moviepy.editor.ipython_display(\"/content/videos/rl-video-episode-1.mp4\")\n"
      ]
    }
  ]
}